- Problems with monolithic architecture?
  - The load balancer can be overwhelmed with the number of concurrent connections?
    (since we have to use websockets or SSE as they are long-lived)
    - Is this actually a problem?
      - Does the load balancer maintain the open connection as well or does it just redirect the client to the
        appropriate pod?
        - If the latter then there is no problem since there is no central bottleneck
          - Load balancers know how to be efficient, this doesn't seem to be a problem
            - https://stackoverflow.com/questions/8627295/websockets-with-load-balancer-scalability
            - https://stackoverflow.com/questions/12526265/loadbalancing-web-sockets
            - https://chat.openai.com/share/c233feb3-2208-4300-ad3e-739b299340f7
      - So probably not an actual problem
  - Ensuring continuity in case there is a server-side failure?
    - Is this actually a problem?
      - Yes
      - If the server crashes while handling a request it will close the websocket/SSE connection
      - If the server fails while handling a request it will close the websocket/SSE connection
    - Is this a large enough problem?
      - When does the server fail?
        - Bug in logic
          - Can't ensure continuity anyway, submission will always fail
        - Runtime conditions (not enough memory, etc.)
          - Probably not occasional enough
          - Can be handled separately
      - So probably not a large enough problem
    - Is a mitigation available in the monolithic approach?
      - Do something like exponential backoff on the client
  - Ensuring continuity in case there is a client-side failure?
    - Is this actually a problem?
      - Yes, if the client loses the websocket/SSE connection for some time, the submission won't be processed
    - Is this a large enough problem?
      - The clients are backend systems not web browsers which should have reliable internet connection, let alone
        the fact that they might be in the same cluster as Envicutor
      - So probably not a large enough problem
      - But the human user is a client to the client, so the client might have to make an asynchronous system with
        health checking which is a downside
  - Large submission objects in per-worker queue
    - Is this actually a problem?
      - Yes, since each worker has its own in-memory queue with the submission objects
    - Is this a large enough problem?
      - If code execution system is used for executing large codebases, the files would increase the size of the
        submission object
        - Rare
    - Is a mitigation available in the monolithic approach?
      - On receiving the request, create the files on disk, queue in memory non-files stuff (limits, etc.)
    - Does the microservices approach actually solve the problem?
      - Yes, since it uses a database (with the large files) + in memory store (with the submission ids)
  - Still have the problem of cache-server installation process shall not be able to access the worker
    - Solve it by running the cache worker installation process in a Docker container that uses a completely isolated
      network
